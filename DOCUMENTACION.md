# Red Neuronal Artificial en C++

## DescripciÃ³n del Proyecto

Este proyecto implementa una **red neuronal artificial multicapa** desde cero en C++, sin utilizar bibliotecas de aprendizaje automÃ¡tico externas. La implementaciÃ³n incluye capas densas (fully connected), funciones de activaciÃ³n (Tanh y Sigmoid), y un algoritmo de entrenamiento basado en retropropagaciÃ³n (backpropagation).

El proyecto estÃ¡ diseÃ±ado con fines educativos para demostrar los fundamentos de las redes neuronales y el aprendizaje profundo.

---

## ğŸ“¹ Video DemostraciÃ³n del Proyecto

**[Ver Video Explicativo Completo](https://drive.google.com/drive/folders/124kfF8cgNvq49oHU7IwM7Y30CCYBtBm2?usp=sharing)**

El video incluye:
- ğŸ¯ ExplicaciÃ³n detallada de la arquitectura de la red neuronal
- ğŸš€ DemostraciÃ³n en vivo del entrenamiento (XOR y dataset grande ~2-3 minutos)
- ğŸ“Š AnÃ¡lisis de resultados y mÃ©tricas de precisiÃ³n
- ğŸ’» Recorrido por el cÃ³digo fuente y explicaciÃ³n de componentes clave
- ğŸ§ª EjecuciÃ³n de pruebas unitarias

---

## CaracterÃ­sticas Principales

- âœ… **ImplementaciÃ³n desde cero**: No depende de bibliotecas como TensorFlow o PyTorch
- âœ… **Clase Matrix personalizada**: Operaciones matriciales bÃ¡sicas implementadas manualmente
- âœ… **Capas modulares**: Arquitectura basada en clases abstractas para fÃ¡cil extensiÃ³n
- âœ… **Funciones de activaciÃ³n**: Tanh y Sigmoid implementadas
- âœ… **RetropropagaciÃ³n**: Algoritmo completo de backpropagation
- âœ… **Pruebas unitarias**: Suite completa de tests para validar componentes
- âœ… **Ejemplo XOR**: DemostraciÃ³n del problema XOR clÃ¡sico

---

## Estructura del Proyecto

```
proyecto-final/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Matrix.h              # Clase para operaciones matriciales
â”‚   â”œâ”€â”€ Network.h/cpp         # Clase principal de la red neuronal
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ Layer.h           # Clase base abstracta para capas
â”‚   â”‚   â”œâ”€â”€ Dense.h/cpp       # Capa densa (fully connected)
â”‚   â”‚   â””â”€â”€ Activation.h      # Funciones de activaciÃ³n (Tanh, Sigmoid)
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ MSE.h             # FunciÃ³n de pÃ©rdida (Mean Squared Error)
â”‚   â””â”€â”€ main.cpp              # Programa principal con ejemplo XOR
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_matrix.cpp       # Tests unitarios para Matrix
â”‚   â”œâ”€â”€ test_dense.cpp        # Tests unitarios para Dense layer
â”‚   â”œâ”€â”€ test_activation.cpp   # Tests unitarios para activaciones
â”‚   â””â”€â”€ test_xor.cpp          # Test de integraciÃ³n con XOR
â”œâ”€â”€ CMakeLists.txt            # ConfiguraciÃ³n de CMake
â””â”€â”€ DOCUMENTACION.md          # Este archivo
```

---

## Requisitos del Sistema

### Compilador
- **GCC 11+** o **Clang 12+** o **MSVC 2019+**
- Soporte para **C++17**

### Herramientas de CompilaciÃ³n
- **CMake 3.18+** (recomendado)
- O compilaciÃ³n directa con g++/clang++

---

## InstalaciÃ³n y CompilaciÃ³n

### OpciÃ³n 1: Usando CMake (Recomendado)

```bash
# Clonar el repositorio
git clone <url-del-repositorio>
cd proyecto-final-2025-2-novo

# Crear directorio de compilaciÃ³n
mkdir build
cd build

# Configurar con CMake
cmake ..

# Compilar
cmake --build .

# Ejecutar el demo
./neural_net_demo

# Ejecutar tests
./test_matrix
./test_dense
./test_activation
./test_xor
```

### OpciÃ³n 2: CompilaciÃ³n Directa con g++

```bash
# Compilar el programa principal
g++ src/main.cpp src/Network.cpp src/layers/Dense.cpp -o neural_net_demo -I src -std=c++17

# Ejecutar
./neural_net_demo

# Compilar tests
g++ tests/test_matrix.cpp -o test_matrix -I src -std=c++17
g++ tests/test_dense.cpp src/Network.cpp src/layers/Dense.cpp -o test_dense -I src -std=c++17
g++ tests/test_activation.cpp -o test_activation -I src -std=c++17
g++ tests/test_xor.cpp src/Network.cpp src/layers/Dense.cpp -o test_xor -I src -std=c++17
```

---

## Uso y Ejemplos

### Ejemplo BÃ¡sico: Problema XOR

El archivo `src/main.cpp` contiene un ejemplo completo del problema XOR:

```cpp
#include "Network.h"
#include "layers/Dense.h"
#include "layers/Activation.h"

int main() {
    // Datos de entrenamiento XOR
    Matrix X(4, 2);
    X.data = {{0, 0}, {0, 1}, {1, 0}, {1, 1}};
    
    Matrix Y(4, 1);
    Y.data = {{0}, {1}, {1}, {0}};
    
    // Crear red neuronal
    Network net;
    net.add(new Dense(2, 3));    // Capa de entrada a oculta
    net.add(new Tanh());         // ActivaciÃ³n
    net.add(new Dense(3, 1));    // Capa oculta a salida
    net.add(new Tanh());         // ActivaciÃ³n de salida
    
    // Entrenar
    net.train(X, Y, 10000, 0.1);
    
    // Predecir
    Matrix prediction = net.predict(X);
    prediction.print();
    
    return 0;
}
```

### Crear una Red Neuronal Personalizada

```cpp
Network net;

// Capa de entrada: 4 neuronas -> 8 neuronas ocultas
net.add(new Dense(4, 8));
net.add(new Tanh());

// Capa oculta: 8 -> 4 neuronas
net.add(new Dense(8, 4));
net.add(new Sigmoid());

// Capa de salida: 4 -> 1 neurona
net.add(new Dense(4, 1));
net.add(new Sigmoid());

// Entrenar con tus datos
net.train(X_train, Y_train, epochs=5000, learning_rate=0.01);

// Hacer predicciones
Matrix predictions = net.predict(X_test);
```

---

## Arquitectura de la SoluciÃ³n

### 1. Clase Matrix

Implementa operaciones matriciales bÃ¡sicas:
- MultiplicaciÃ³n de matrices
- TransposiciÃ³n
- Suma y resta
- MultiplicaciÃ³n por escalar
- Producto de Hadamard (elemento por elemento)

### 2. Capas (Layers)

**Clase Base: Layer**
```cpp
class Layer {
    virtual Matrix forward(const Matrix& input) = 0;
    virtual Matrix backward(const Matrix& output_gradient, double learning_rate) = 0;
};
```

**Dense Layer (Capa Densa)**
- Implementa una capa totalmente conectada
- Forward pass: `Y = X * W + B`
- Backward pass: Calcula gradientes para pesos, bias y entrada
- Actualiza parÃ¡metros usando descenso de gradiente

**Activation Layer (Capa de ActivaciÃ³n)**
- Aplica funciones no lineales
- Tanh: `f(x) = tanh(x)`, `f'(x) = 1 - tanhÂ²(x)`
- Sigmoid: `f(x) = 1/(1+e^-x)`, `f'(x) = f(x)(1-f(x))`

### 3. Network (Red Neuronal)

Gestiona mÃºltiples capas:
- `add(Layer*)`: AÃ±ade una capa a la red
- `predict(Matrix)`: PropagaciÃ³n hacia adelante
- `train(X, Y, epochs, lr)`: Entrenamiento con backpropagation

### 4. FunciÃ³n de PÃ©rdida

**Mean Squared Error (MSE)**
- `loss(y_true, y_pred)`: Calcula el error
- `prime(y_true, y_pred)`: Calcula el gradiente

---

## Pruebas Unitarias

### Test de Matrix
- CreaciÃ³n de matrices
- MultiplicaciÃ³n de matrices
- TransposiciÃ³n
- Operaciones aritmÃ©ticas
- Producto de Hadamard

### Test de Dense Layer
- Forward pass con valores conocidos
- Backward pass y actualizaciÃ³n de gradientes
- VerificaciÃ³n de dimensiones

### Test de Activaciones
- Forward pass de Tanh y Sigmoid
- Backward pass y derivadas
- VerificaciÃ³n de rangos de salida

### Test de IntegraciÃ³n XOR
- Entrenamiento completo en problema XOR
- VerificaciÃ³n de convergencia
- ValidaciÃ³n de predicciones

---

## Fundamentos TeÃ³ricos

### RetropropagaciÃ³n (Backpropagation)

El algoritmo de backpropagation se basa en la regla de la cadena del cÃ¡lculo:

1. **Forward Pass**: Calcular la salida de la red
2. **Calcular Error**: Comparar con el valor esperado
3. **Backward Pass**: Propagar el gradiente hacia atrÃ¡s
4. **Actualizar Pesos**: `W = W - Î± * âˆ‡W`

### Descenso de Gradiente

La actualizaciÃ³n de parÃ¡metros sigue la fÃ³rmula:
```
Î¸_nuevo = Î¸_viejo - Î± * âˆ‚L/âˆ‚Î¸
```

Donde:
- `Î¸`: ParÃ¡metros (pesos y bias)
- `Î±`: Tasa de aprendizaje (learning rate)
- `âˆ‚L/âˆ‚Î¸`: Gradiente de la funciÃ³n de pÃ©rdida

---

## AnÃ¡lisis de Rendimiento

### Problema XOR

**ConfiguraciÃ³n:**
- Arquitectura: 2 â†’ 3 â†’ 1 (con Tanh)
- Ã‰pocas: 10,000
- Learning rate: 0.1

**Resultados:**
- Convergencia: ~5,000 Ã©pocas
- Error final: < 0.01
- PrecisiÃ³n: 100% en clasificaciÃ³n binaria

### Ventajas

âœ… CÃ³digo educativo y fÃ¡cil de entender  
âœ… Sin dependencias externas complejas  
âœ… Arquitectura modular y extensible  
âœ… Tests completos para validaciÃ³n  

### Limitaciones

âš ï¸ No optimizado para grandes datasets  
âš ï¸ Sin paralelizaciÃ³n (CPU single-thread)  
âš ï¸ Operaciones matriciales no optimizadas  
âš ï¸ Solo funciones de activaciÃ³n bÃ¡sicas  

---

## Mejoras Futuras

### Optimizaciones de Rendimiento
- [ ] Usar BLAS/LAPACK para operaciones matriciales
- [ ] Implementar mini-batch gradient descent
- [ ] ParalelizaciÃ³n con OpenMP o threads
- [ ] Optimizaciones de memoria

### Nuevas CaracterÃ­sticas
- [ ] MÃ¡s funciones de activaciÃ³n (ReLU, Leaky ReLU, Softmax)
- [ ] MÃ¡s funciones de pÃ©rdida (Cross-Entropy, Hinge Loss)
- [ ] Optimizadores avanzados (Adam, RMSprop, Momentum)
- [ ] RegularizaciÃ³n (L1, L2, Dropout)
- [ ] Capas convolucionales (CNN)
- [ ] Guardado/carga de modelos

### Herramientas
- [ ] VisualizaciÃ³n de pÃ©rdida durante entrenamiento
- [ ] ExportaciÃ³n de mÃ©tricas
- [ ] Interfaz de lÃ­nea de comandos mejorada

---

## SoluciÃ³n de Problemas

### Error: "cmake: command not found"
**SoluciÃ³n**: Usa compilaciÃ³n directa con g++ (ver secciÃ³n de instalaciÃ³n)

### Error de compilaciÃ³n con g++
**SoluciÃ³n**: AsegÃºrate de tener C++17:
```bash
g++ --version  # Verifica versiÃ³n
g++ -std=c++17 ...  # Especifica estÃ¡ndar
```

### La red no converge
**Soluciones**:
- Reducir learning rate (ej: 0.01 en vez de 0.1)
- Aumentar nÃºmero de Ã©pocas
- AÃ±adir mÃ¡s neuronas en capa oculta
- Cambiar inicializaciÃ³n de pesos
- Normalizar datos de entrada

---

## Contribuciones

Este es un proyecto acadÃ©mico. Si deseas contribuir:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-caracteristica`)
3. Commit tus cambios (`git commit -am 'AÃ±adir nueva caracterÃ­stica'`)
4. Push a la rama (`git push origin feature/nueva-caracteristica`)
5. Crea un Pull Request

---

## Licencia

Este proyecto usa la licencia **MIT**. Ver archivo LICENSE para mÃ¡s detalles.

---

## Referencias BibliogrÃ¡ficas

[1] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*. MIT Press, 2016.

[2] M. Nielsen, "Neural Networks and Deep Learning," Determination Press, 2015. [Online]. Available: http://neuralnetworksanddeeplearning.com/

[3] C. M. Bishop, *Pattern Recognition and Machine Learning*. Springer, 2006.

[4] Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," *Nature*, vol. 521, no. 7553, pp. 436-444, 2015.

---

## Contacto y Soporte

Para preguntas o problemas, por favor abre un issue en el repositorio de GitHub.

**Grupo**: group_3_custom_name  
**Curso**: CS2013 ProgramaciÃ³n III  
**AÃ±o**: 2025-1
